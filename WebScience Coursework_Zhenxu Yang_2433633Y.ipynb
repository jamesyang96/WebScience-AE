{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy\n",
    "!pip install TextBlob\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import json\n",
    "import emoji as em\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    " \n",
    "import twitter_credentials\n",
    " \n",
    "# # # # TWITTER STREAMER # # # #\n",
    "class TwitterStreamer():\n",
    "    \"\"\"\n",
    "    Class for streaming and processing live tweets.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def stream_tweets(self, fetched_tweets_filename, hash_tag_list):\n",
    "        # This handles Twitter authetification and the connection to Twitter Streaming API\n",
    "        listener = StdOutListener(fetched_tweets_filename)\n",
    "        auth = OAuthHandler(twitter_credentials.CONSUMER_KEY, twitter_credentials.CONSUMER_SECRET)\n",
    "        auth.set_access_token(twitter_credentials.ACCESS_TOKEN, twitter_credentials.ACCESS_TOKEN_SECRET)\n",
    "        stream = Stream(auth, listener)\n",
    "\n",
    "        # This line filter Twitter Streams to capture data by the keywords: \n",
    "        stream.filter(track = hash_tag_list)\n",
    "\n",
    "\n",
    "# # # # TWITTER STREAM LISTENER # # # #\n",
    "class StdOutListener(StreamListener):\n",
    "    \"\"\"\n",
    "    This is a basic listener that just prints received tweets to stdout.\n",
    "    \"\"\"\n",
    "    def __init__(self, fetched_tweets_filename):\n",
    "        self.fetched_tweets_filename = fetched_tweets_filename\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            #print(data)\n",
    "            print('Running...')\n",
    "            with open(self.fetched_tweets_filename, 'a') as tf:\n",
    "                tf.write(data)\n",
    "            return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data %s\" % str(e))\n",
    "        return True\n",
    "          \n",
    "\n",
    "    def on_error(self, status):\n",
    "        if status == 420:\n",
    "            # Returning False on_data method in case rate limit occurs.\n",
    "            return False\n",
    "        print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reading data from json file\n",
    "def saveToDataFrame(localfile):\n",
    "    tweets_tmp = []\n",
    "    thread_all = []\n",
    "\n",
    "    with open(localfile) as jsonfile:\n",
    "        for i, line in enumerate(jsonfile):\n",
    "            thread = json.loads(line)\n",
    "            thread_all.append(thread)\n",
    "\n",
    "            if all (k in thread for k in ('id','created_at', 'extended_tweet', 'entities','lang')):\n",
    "                if len(thread['extended_tweet']['entities']['hashtags']) == 0:\n",
    "                    tweets_tmp.append((thread['id'], \n",
    "                                       thread['created_at'], \n",
    "                                       thread['extended_tweet']['full_text'],\n",
    "                                       len(thread['extended_tweet']['full_text']), \n",
    "                                       '0', \n",
    "                                       thread['lang']\n",
    "                                      ))\n",
    "\n",
    "                else:\n",
    "                    tweets_tmp.append((thread['id'], \n",
    "                                       thread['created_at'], \n",
    "                                       thread['extended_tweet']['full_text'],\n",
    "                                       len(thread['extended_tweet']['full_text']),\n",
    "                                       [tag['text'] for tag in thread['extended_tweet']['entities']['hashtags']],\n",
    "                                       thread['lang']\n",
    "                                      ))\n",
    "\n",
    "\n",
    "            elif all (k in thread for k in ('id','created_at', 'text', 'entities')):\n",
    "                if len(thread['entities']['hashtags']) == 0:\n",
    "                    tweets_tmp.append((thread['id'], \n",
    "                                       thread['created_at'], \n",
    "                                       thread['text'],\n",
    "                                       len(thread['text']), \n",
    "                                       '0',\n",
    "                                       thread['lang']\n",
    "                                      ))\n",
    "\n",
    "                else:\n",
    "                    tweets_tmp.append((thread['id'], \n",
    "                                       thread['created_at'], \n",
    "                                       thread['text'], \n",
    "                                       len(thread['text']), \n",
    "                                       [tag['text'] for tag in thread['entities']['hashtags']],\n",
    "                                       thread['lang']\n",
    "                                      ))\n",
    "\n",
    "    #print(len(tweets_tmp))\n",
    "\n",
    "    # Create the tweets data frame.  \n",
    "    labels = ['id', 'created_at', 'tweets', 'tweets_len', 'hashtags','language']\n",
    "    tweets_frame = pd.DataFrame(tweets_tmp, columns=labels)\n",
    "\n",
    "    tweets_frame = tweets_frame[tweets_frame.language == 'en'] #filter english\n",
    "    \n",
    "    tweets_frame = tweets_frame.drop_duplicates(subset='tweets', keep='first', inplace=False) #drop duplicate\n",
    "    \n",
    "    tweets_frame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"Number of Tweets got: \", len(tweets_frame))\n",
    "\n",
    "    #tweets_frame.head()\n",
    "    \n",
    "    \n",
    "    return tweets_frame, thread_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization, normalization, remove stop words, stemming, lemmatization\n",
    "#Modify from code in Text as Data course\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
    "nlp.remove_pipe('tagger')\n",
    "nlp.remove_pipe('parser')\n",
    "\n",
    "#@Tokenize\n",
    "def spacy_tokenize(string):\n",
    "    tokens = []\n",
    "    doc = nlp(string)\n",
    "    for token in doc:\n",
    "        tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "#@Normalize\n",
    "def normalize(tokens):\n",
    "    normalized_tokens = []\n",
    "    for token in tokens:\n",
    "        normalized = token.text.lower().strip()\n",
    "        if ((token.is_alpha or token.is_digit)):\n",
    "            normalized_tokens.append(normalized)\n",
    "    return normalized_tokens\n",
    "\n",
    "def spellCorrection(normalized_tokens):\n",
    "    corrected = TextBlob(' '.join(normalized_tokens))\n",
    "    \n",
    "    return str(corrected.correct()).split()\n",
    "\n",
    "#Remove Stop Words\n",
    "def removeStopWord(normalized_tokens):\n",
    "    filtered_sentence =[]\n",
    "    for word in normalized_tokens:\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word)   \n",
    "    return filtered_sentence\n",
    "\n",
    "#Stemming using PorterStemmer\n",
    "def stemming(filtered_sentence):\n",
    "    stemmed_sentence = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for word in filtered_sentence:\n",
    "        token = stemmer.stem(word)\n",
    "        stemmed_sentence.append(token)\n",
    "    return ' '.join(stemmed_sentence)\n",
    "\n",
    "#lemming\n",
    "def lemming(filtered_sentence):\n",
    "    lemmed_sentence = []\n",
    "    tokens = nlp(' '.join(filtered_sentence))\n",
    "    for token in tokens:\n",
    "        lemmed_sentence.append(token.lemma_)\n",
    "    return ' '.join(lemmed_sentence)\n",
    "\n",
    "#@Tokenize, normalize, and lemming\n",
    "def tokenize_normalize_lem(string):\n",
    "    return lemming(normalize(spacy_tokenize(string)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling excitement, happy, pleasant, surprise, fear, angry data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not run crawl codes in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all hashtag list\n",
    "hash_tag_list = ['#excitement', '#exciting', '#excited', '#thrilled', '#amazing',\n",
    "                 '#Happy', '#happiness', '#joy', '#love', '#cheerful', '#delighted',  '#laughing',\n",
    "                 '#pleasant', '#glad', '#satisfied', '#appreciated', '#appreciate',\n",
    "                 '#shock', '#sad', '#frustration', '#frustrated',\n",
    "                 '#fear', '#disgust', '#depressed', '#depression', '#afraid', '#scary', '#awful',\n",
    "                 '#angry', '#pissed', '#furious', '#outraged', '#indignant', \n",
    "                 em.emojize(':relaxed:', use_aliases=True),\n",
    "                 em.emojize(':pensive:', use_aliases=True),\n",
    "                 em.emojize(':cry:', use_aliases=True),\n",
    "                 em.emojize(':sob:', use_aliases=True),\n",
    "                 em.emojize(':fearful:', use_aliases=True),\n",
    "                 em.emojize(':anger:', use_aliases=True),\n",
    "                 em.emojize(':angry:', use_aliases=True)]\n",
    "\n",
    "twitter_streamer = TwitterStreamer()\n",
    "#twitter_streamer.stream_tweets('tweets_all.json', hash_tag_list)\n",
    "twitter_streamer.stream_tweets('sample_tweets.json', hash_tag_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawling excitement, done\n",
    "hash_tag_list = ['#excitement', '#exciting', '#excited', '#thrilled', '#amazing']\n",
    "\n",
    "twitter_streamer = TwitterStreamer()\n",
    "twitter_streamer.stream_tweets('tweets_excitement.json', hash_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawling happy, done, 51 mins\n",
    "hash_tag_list = ['#Happy', '#happiness', '#joy', '#love', '#cheerful', '#delighted',  '#laughing']\n",
    "\n",
    "twitter_streamer = TwitterStreamer()\n",
    "twitter_streamer.stream_tweets('tweets_happy.json', hash_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawling pleasant, done\n",
    "hash_tag_list = ['#pleasant', '#glad', '#satisfied', '#appreciated', '#appreciate', \n",
    "                 em.emojize(':relaxed:', use_aliases=True)]\n",
    "\n",
    "twitter_streamer = TwitterStreamer()\n",
    "twitter_streamer.stream_tweets('tweets_pleasant.json', hash_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawling surprise, done\n",
    "hash_tag_list = ['#shock', '#sad', '#frustration', '#frustrated', \n",
    "                 em.emojize(':pensive:', use_aliases=True),\n",
    "                 em.emojize(':cry:', use_aliases=True),\n",
    "                 em.emojize(':sob:', use_aliases=True)]\n",
    "\n",
    "twitter_streamer = TwitterStreamer()\n",
    "twitter_streamer.stream_tweets('tweets_surprise.json', hash_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawling fear, done\n",
    "hash_tag_list = ['#fear', '#disgust', '#depressed', '#depression', '#afraid', '#scary', '#awful', \n",
    "                 em.emojize(':fearful:', use_aliases=True)]\n",
    "\n",
    "twitter_streamer = TwitterStreamer()\n",
    "twitter_streamer.stream_tweets('tweets_fear.json', hash_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawling Angry, done\n",
    "hash_tag_list = ['#angry', '#pissed', '#furious', '#outraged', '#indignant', \n",
    "                 em.emojize(':anger:', use_aliases=True),\n",
    "                 em.emojize(':angry:', use_aliases=True)]\n",
    "\n",
    "twitter_streamer = TwitterStreamer()\n",
    "twitter_streamer.stream_tweets('tweets_angry.json', hash_tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tweets got:  70941\n"
     ]
    }
   ],
   "source": [
    "#Read\n",
    "#tweets_all, _ = saveToDataFrame('tweets_all.json')\n",
    "tweets_all, _ = saveToDataFrame('sample_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract emoji\n",
    "#Citation: https://stackoverflow.com/questions/43146528/how-to-extract-all-the-emojis-from-text\n",
    "\n",
    "def extract_emojis(str):\n",
    "    return ''.join(c for c in str if c in em.UNICODE_EMOJI)\n",
    "\n",
    "tweets_all['emoji'] = np.array([extract_emojis(tweet) for tweet in tweets_all['tweets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean tweets\n",
    "#Citation\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "\n",
    "tweets_all['clean_tweents'] = np.array([clean_tweet(tweet) for tweet in tweets_all['tweets']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment analysis to tell the text is possitive, neutral, or negative.\n",
    "#Citation: https://github.com/vprusso/youtube_tutorials/blob/master/twitter_python/part_5_sentiment_analysis_tweet_data/sentiment_anaylsis_twitter_data.py\n",
    "\n",
    "def analyze_sentiment(tweet):\n",
    "        analysis = TextBlob(tweet)\n",
    "        \n",
    "        if analysis.sentiment.polarity > 0:\n",
    "            return 1\n",
    "        elif analysis.sentiment.polarity == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "tweets_all['sentiment'] = np.array([analyze_sentiment(tweet) for tweet in tweets_all['clean_tweents']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply tokenization, normalization, spelling correction, and lemmatization\n",
    "tweets_all['clean_tweets_lemmed'] = np.array([tokenize_normalize_lem(tweet) for tweet in tweets_all['tweets']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicate the third time\n",
    "#Drop duplicate the second time\n",
    "tweets_all.drop_duplicates(subset='tweets', keep='first', inplace=True)\n",
    "tweets_all.drop_duplicates(subset='clean_tweets_lemmed', keep='first', inplace=True)\n",
    "tweets_all.drop_duplicates(subset='clean_tweents', keep='first', inplace=True)\n",
    "tweets_all.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67610"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop more duplicates, expecially those with same text but different web link:\n",
    "len(tweets_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalized the hashtags\n",
    "def lowertag(tags):\n",
    "    lowered_tags = []\n",
    "    if tags == '0':\n",
    "        lowered_tags.append(tags)\n",
    "    else:\n",
    "        for i in range(len(tags)):\n",
    "            tag = str(tags[i])\n",
    "            lowered = tag.lower().strip()\n",
    "            lowered_tags.append(lowered)\n",
    "    return lowered_tags\n",
    "\n",
    "tweets_all['cleaned_tags'] = np.array([lowertag(tag) for tag in tweets_all.hashtags.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hashtag counts\n",
    "def countHashtags(hashlist):\n",
    "    counts = []\n",
    "    for tags in tweets_all.cleaned_tags:\n",
    "        count = 0\n",
    "        for i in tags:\n",
    "            if i in hashlist:\n",
    "                count += 1\n",
    "        counts.append(count)\n",
    "    \n",
    "    return np.array(counts)\n",
    "\n",
    "#emoji counts\n",
    "def countEmojis(emojilist):\n",
    "    counts = []\n",
    "    for emojis in tweets_all.emoji:\n",
    "        count = 0\n",
    "        for i in emojis:\n",
    "            if i in emojilist:\n",
    "                count += 1\n",
    "        counts.append(count)\n",
    "    \n",
    "    return np.array(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_all['excitement'] = countHashtags(['excitement', 'exciting', 'excited', \n",
    "                                                   'thrilled', 'amazing']) + countEmojis([\n",
    "    em.emojize(':laughing:', use_aliases=True),\n",
    "    em.emojize(':grimacing:', use_aliases=True),\n",
    "    em.emojize(':grinning:', use_aliases=True)])\n",
    "\n",
    "tweets_all['happy'] = countHashtags(['happy', 'happiness', 'joy', 'love', \n",
    "                                              'cheerful', 'delighted',  'laughing']) + countEmojis([\n",
    "    em.emojize(':kissing_closed_eyes:', use_aliases=True),\n",
    "    em.emojize(':joy:', use_aliases=True)])\n",
    "\n",
    "tweets_all['pleasant'] = countHashtags(['pleasant', 'glad', 'satisfied', \n",
    "                                                 'appreciated', 'appreciate']) + countEmojis([\n",
    "    em.emojize(':relaxed:', use_aliases=True),\n",
    "    em.emojize(':satisfied:', use_aliases=True),\n",
    "    em.emojize(':simple_smile:', use_aliases=True)])\n",
    "\n",
    "tweets_all['surprise'] = countHashtags(['shock', 'sad', 'frustration', 'frustrated']) + countEmojis([\n",
    "    em.emojize(':pensive:', use_aliases=True), \n",
    "    em.emojize(':cry:', use_aliases=True),\n",
    "    em.emojize(':sob:', use_aliases=True)])\n",
    "\n",
    "tweets_all['fear'] = countHashtags(['fear', 'disgust', 'depressed', 'depression', \n",
    "                                    'afraid', 'scary', 'awful']) + countEmojis([\n",
    "    em.emojize(':fearful:', use_aliases=True),\n",
    "    em.emojize(':scream:', use_aliases=True),\n",
    "    em.emojize(':sleepy:', use_aliases=True)])\n",
    "\n",
    "tweets_all['angry'] = countHashtags(['angry', 'pissed', 'furious', 'outraged', 'indignant']) + countEmojis([\n",
    "    em.emojize(':anger:', use_aliases=True), \n",
    "    em.emojize(':angry:', use_aliases=True),\n",
    "    em.emojize(':rage:', use_aliases=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Labeling\n",
    "#according to score\n",
    "labeldata = tweets_all[['excitement', 'happy','pleasant', 'surprise', 'fear', 'angry']]\n",
    "newlabels = labeldata.idxmax(axis = 1)\n",
    "#deal with 0, if 0, keepe original label\n",
    "for i in range(len(tweets_all)):\n",
    "    if labeldata.sum(axis = 1)[i] == 0:\n",
    "        newlabels[i] = '0'\n",
    "\n",
    "tweets_all['label'] = newlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_all = tweets_all[~(tweets_all['label'] == '0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep those longger than 40 characters\n",
    "tweets_all = tweets_all[tweets_all.tweets_len > 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing angry label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "angry = tweets_all[tweets_all.label == 'angry']\n",
    "angry.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#angry should be negative emotion, filter using sentiment <= 0\n",
    "angry = angry[angry.sentiment <= 0]\n",
    "angry.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "617"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(angry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fear = tweets_all[tweets_all.label == 'fear']\n",
    "fear.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fear should be negative emotion, filter using sentiment <= 0\n",
    "fear = fear[fear.sentiment <= 0]\n",
    "fear.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "685"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprise = tweets_all[tweets_all.label == 'surprise']\n",
    "surprise.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should be negative feelings, filter sentiment, delet sentiment>0(possitive)\n",
    "surprise = surprise[surprise.sentiment <= 0]\n",
    "surprise.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15346"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(surprise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing pleasant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pleasant = tweets_all[tweets_all.label == 'pleasant']\n",
    "pleasant.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pleasant should be possitive emotion, filter using sentiment >= 0\n",
    "pleasant = pleasant[pleasant.sentiment >= 0]\n",
    "pleasant.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pleasant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy = tweets_all[tweets_all.label == 'happy']\n",
    "happy.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#happy should be possitive, filter with sentiment > 0(excluding 0)\n",
    "happy = happy[happy.sentiment > 0]\n",
    "happy.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7663"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(happy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing excitment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "excitement = tweets_all[tweets_all.label == 'excitement']\n",
    "excitement.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#should be possitive, filter with sentiment > 0(excluding 0)\n",
    "excitement = excitement[excitement.sentiment > 0]\n",
    "excitement.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1117"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(excitement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>tweets</th>\n",
       "      <th>tweets_len</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>language</th>\n",
       "      <th>emoji</th>\n",
       "      <th>clean_tweents</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_tweets_lemmed</th>\n",
       "      <th>cleaned_tags</th>\n",
       "      <th>excitement</th>\n",
       "      <th>happy</th>\n",
       "      <th>pleasant</th>\n",
       "      <th>surprise</th>\n",
       "      <th>fear</th>\n",
       "      <th>angry</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1236043548459274240</td>\n",
       "      <td>Fri Mar 06 21:38:41 +0000 2020</td>\n",
       "      <td>@buddyboi94 @deannamarsh751 @BrandonSunday23 A...</td>\n",
       "      <td>92</td>\n",
       "      <td>[Excited]</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>And a new department joins the lineup Excited</td>\n",
       "      <td>1</td>\n",
       "      <td>and a new department join the lineup excite</td>\n",
       "      <td>[excited]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>excitement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1236043554788487168</td>\n",
       "      <td>Fri Mar 06 21:38:42 +0000 2020</td>\n",
       "      <td>RT @lilyachty: Just woke up bout to dive into ...</td>\n",
       "      <td>75</td>\n",
       "      <td>[goodday, excited]</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>RT Just woke up bout to dive into goodday excited</td>\n",
       "      <td>1</td>\n",
       "      <td>rt just wake up bout to diva into goodday excite</td>\n",
       "      <td>[goodday, excited]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>excitement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1236043596480106497</td>\n",
       "      <td>Fri Mar 06 21:38:52 +0000 2020</td>\n",
       "      <td>It's : 2020-03-06T21:38:52.1734695Z \\n\\n#tuesd...</td>\n",
       "      <td>230</td>\n",
       "      <td>[tuesdaymotivation, funny, pets, followme, f4f...</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>It s 2020 03 06T21 38 52 1734695Z tuesdaymotiv...</td>\n",
       "      <td>1</td>\n",
       "      <td>it 2020 03 tuesdaymotivation funny pet followm...</td>\n",
       "      <td>[tuesdaymotivation, funny, pets, followme, f4f...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>excitement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1236043643363852288</td>\n",
       "      <td>Fri Mar 06 21:39:03 +0000 2020</td>\n",
       "      <td>RT @AuthorAaronHowe: REMINDER: You deserve #Ha...</td>\n",
       "      <td>72</td>\n",
       "      <td>[Happiness, Success]</td>\n",
       "      <td>en</td>\n",
       "      <td>ðŸ˜€</td>\n",
       "      <td>RT REMINDER You deserve Happiness and Success</td>\n",
       "      <td>1</td>\n",
       "      <td>rt reminder you deserve happiness and success</td>\n",
       "      <td>[happiness, success]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>excitement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1236043662842368004</td>\n",
       "      <td>Fri Mar 06 21:39:08 +0000 2020</td>\n",
       "      <td>silverstein and four year strong tonight with ...</td>\n",
       "      <td>82</td>\n",
       "      <td>[amazing]</td>\n",
       "      <td>en</td>\n",
       "      <td></td>\n",
       "      <td>silverstein and four year strong tonight with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>silverstein and four year strong tonight with ...</td>\n",
       "      <td>[amazing]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>excitement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>1236104067501314048</td>\n",
       "      <td>Sat Mar 07 01:39:10 +0000 2020</td>\n",
       "      <td>Well done, Mr. Bloomberg!\\nWorth watching! Twi...</td>\n",
       "      <td>78</td>\n",
       "      <td>[TrumpSUCKS, VoteBlue2020]</td>\n",
       "      <td>en</td>\n",
       "      <td>ðŸ˜†</td>\n",
       "      <td>Well done Mr Bloomberg Worth watching Twice Tr...</td>\n",
       "      <td>1</td>\n",
       "      <td>good do bloomberg worth watch twice trumpsucks</td>\n",
       "      <td>[trumpsucks, voteblue2020]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>excitement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>1236104070013710336</td>\n",
       "      <td>Sat Mar 07 01:39:10 +0000 2020</td>\n",
       "      <td>@gibsonfilms @JosephBiwald That skull pattern ...</td>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>ðŸ˜¬</td>\n",
       "      <td>That skull pattern is killer on the Hunter hel...</td>\n",
       "      <td>1</td>\n",
       "      <td>that skull pattern be killer on the hunter hel...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>excitement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>1236104072421240832</td>\n",
       "      <td>Sat Mar 07 01:39:11 +0000 2020</td>\n",
       "      <td>Lol been home for a hour now &amp;amp; all Iâ€™ve do...</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>ðŸ˜†</td>\n",
       "      <td>Lol been home for a hour now amp all I ve done...</td>\n",
       "      <td>1</td>\n",
       "      <td>lol be home for a hour now amp all i do be tak...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>excitement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>1236104092444962817</td>\n",
       "      <td>Sat Mar 07 01:39:15 +0000 2020</td>\n",
       "      <td>ECCC got postponed and I'm bummed but look! Th...</td>\n",
       "      <td>279</td>\n",
       "      <td>[CaptainAmerica, SteveRogers, Funko, FunkoPop,...</td>\n",
       "      <td>en</td>\n",
       "      <td>ðŸ˜†ðŸ‡ºðŸ‡¸</td>\n",
       "      <td>ECCC got postponed and I m bummed but look Thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>eccc get postpone and i bum but look this be l...</td>\n",
       "      <td>[captainamerica, steverogers, funko, funkopop,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>excitement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>1236104114360156160</td>\n",
       "      <td>Sat Mar 07 01:39:21 +0000 2020</td>\n",
       "      <td>@LucyWhiteDublin @fliceverett Mia Farrow was t...</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>ðŸ˜†</td>\n",
       "      <td>Mia Farrow was twenty one when she married the...</td>\n",
       "      <td>1</td>\n",
       "      <td>mia farrow be twenty one when she marry the fi...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>excitement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1117 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                      created_at  \\\n",
       "0     1236043548459274240  Fri Mar 06 21:38:41 +0000 2020   \n",
       "1     1236043554788487168  Fri Mar 06 21:38:42 +0000 2020   \n",
       "2     1236043596480106497  Fri Mar 06 21:38:52 +0000 2020   \n",
       "3     1236043643363852288  Fri Mar 06 21:39:03 +0000 2020   \n",
       "4     1236043662842368004  Fri Mar 06 21:39:08 +0000 2020   \n",
       "...                   ...                             ...   \n",
       "1112  1236104067501314048  Sat Mar 07 01:39:10 +0000 2020   \n",
       "1113  1236104070013710336  Sat Mar 07 01:39:10 +0000 2020   \n",
       "1114  1236104072421240832  Sat Mar 07 01:39:11 +0000 2020   \n",
       "1115  1236104092444962817  Sat Mar 07 01:39:15 +0000 2020   \n",
       "1116  1236104114360156160  Sat Mar 07 01:39:21 +0000 2020   \n",
       "\n",
       "                                                 tweets  tweets_len  \\\n",
       "0     @buddyboi94 @deannamarsh751 @BrandonSunday23 A...          92   \n",
       "1     RT @lilyachty: Just woke up bout to dive into ...          75   \n",
       "2     It's : 2020-03-06T21:38:52.1734695Z \\n\\n#tuesd...         230   \n",
       "3     RT @AuthorAaronHowe: REMINDER: You deserve #Ha...          72   \n",
       "4     silverstein and four year strong tonight with ...          82   \n",
       "...                                                 ...         ...   \n",
       "1112  Well done, Mr. Bloomberg!\\nWorth watching! Twi...          78   \n",
       "1113  @gibsonfilms @JosephBiwald That skull pattern ...         267   \n",
       "1114  Lol been home for a hour now &amp; all Iâ€™ve do...          88   \n",
       "1115  ECCC got postponed and I'm bummed but look! Th...         279   \n",
       "1116  @LucyWhiteDublin @fliceverett Mia Farrow was t...         188   \n",
       "\n",
       "                                               hashtags language emoji  \\\n",
       "0                                             [Excited]       en         \n",
       "1                                    [goodday, excited]       en         \n",
       "2     [tuesdaymotivation, funny, pets, followme, f4f...       en         \n",
       "3                                  [Happiness, Success]       en     ðŸ˜€   \n",
       "4                                             [amazing]       en         \n",
       "...                                                 ...      ...   ...   \n",
       "1112                         [TrumpSUCKS, VoteBlue2020]       en     ðŸ˜†   \n",
       "1113                                                  0       en     ðŸ˜¬   \n",
       "1114                                                  0       en     ðŸ˜†   \n",
       "1115  [CaptainAmerica, SteveRogers, Funko, FunkoPop,...       en   ðŸ˜†ðŸ‡ºðŸ‡¸   \n",
       "1116                                                  0       en     ðŸ˜†   \n",
       "\n",
       "                                          clean_tweents  sentiment  \\\n",
       "0         And a new department joins the lineup Excited          1   \n",
       "1     RT Just woke up bout to dive into goodday excited          1   \n",
       "2     It s 2020 03 06T21 38 52 1734695Z tuesdaymotiv...          1   \n",
       "3         RT REMINDER You deserve Happiness and Success          1   \n",
       "4     silverstein and four year strong tonight with ...          1   \n",
       "...                                                 ...        ...   \n",
       "1112  Well done Mr Bloomberg Worth watching Twice Tr...          1   \n",
       "1113  That skull pattern is killer on the Hunter hel...          1   \n",
       "1114  Lol been home for a hour now amp all I ve done...          1   \n",
       "1115  ECCC got postponed and I m bummed but look Thi...          1   \n",
       "1116  Mia Farrow was twenty one when she married the...          1   \n",
       "\n",
       "                                    clean_tweets_lemmed  \\\n",
       "0           and a new department join the lineup excite   \n",
       "1      rt just wake up bout to diva into goodday excite   \n",
       "2     it 2020 03 tuesdaymotivation funny pet followm...   \n",
       "3         rt reminder you deserve happiness and success   \n",
       "4     silverstein and four year strong tonight with ...   \n",
       "...                                                 ...   \n",
       "1112     good do bloomberg worth watch twice trumpsucks   \n",
       "1113  that skull pattern be killer on the hunter hel...   \n",
       "1114  lol be home for a hour now amp all i do be tak...   \n",
       "1115  eccc get postpone and i bum but look this be l...   \n",
       "1116  mia farrow be twenty one when she marry the fi...   \n",
       "\n",
       "                                           cleaned_tags  excitement  happy  \\\n",
       "0                                             [excited]           1      0   \n",
       "1                                    [goodday, excited]           1      0   \n",
       "2     [tuesdaymotivation, funny, pets, followme, f4f...           1      1   \n",
       "3                                  [happiness, success]           1      1   \n",
       "4                                             [amazing]           1      0   \n",
       "...                                                 ...         ...    ...   \n",
       "1112                         [trumpsucks, voteblue2020]           1      0   \n",
       "1113                                                [0]           1      0   \n",
       "1114                                                [0]           1      0   \n",
       "1115  [captainamerica, steverogers, funko, funkopop,...           1      0   \n",
       "1116                                                [0]           1      0   \n",
       "\n",
       "      pleasant  surprise  fear  angry       label  \n",
       "0            0         0     0      0  excitement  \n",
       "1            0         0     0      0  excitement  \n",
       "2            0         0     0      0  excitement  \n",
       "3            0         0     0      0  excitement  \n",
       "4            0         0     0      0  excitement  \n",
       "...        ...       ...   ...    ...         ...  \n",
       "1112         1         0     0      0  excitement  \n",
       "1113         0         0     0      0  excitement  \n",
       "1114         1         0     0      0  excitement  \n",
       "1115         1         0     0      0  excitement  \n",
       "1116         1         0     0      0  excitement  \n",
       "\n",
       "[1117 rows x 18 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Data & Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tweets_all = tweets_all\n",
    "final_tweets_all.to_csv('final_data/final_tweets_all.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_excitement = excitement[['id', 'tweets', 'created_at']]\n",
    "final_happy = happy[['id', 'tweets', 'created_at']]\n",
    "final_pleasant = pleasant[['id', 'tweets', 'created_at']]\n",
    "final_surprise = surprise[['id', 'tweets', 'created_at']]\n",
    "final_fear = fear[['id', 'tweets', 'created_at']]\n",
    "final_angry = angry[['id', 'tweets', 'created_at']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_excitement.to_csv('final_data/excitement.csv', index = False)\n",
    "final_happy.to_csv('final_data/happy.csv', index = False)\n",
    "final_pleasant.to_csv('final_data/pleasant.csv', index = False)\n",
    "final_surprise.to_csv('final_data/furprise.csv', index = False)\n",
    "final_fear.to_csv('final_data/fear.csv', index = False)\n",
    "final_angry.to_csv('final_data/angry.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crowdsorsing_data = pd.concat([excitement[['tweets', 'label']].sample(n=30), \n",
    "                               happy[['tweets', 'label']].sample(n=30), \n",
    "                               pleasant[['tweets', 'label']].sample(n=30), \n",
    "                               surprise[['tweets', 'label']].sample(n=30), \n",
    "                               fear[['tweets', 'label']].sample(n=30), \n",
    "                               angry[['tweets', 'label']].sample(n=30)], ignore_index=True)\n",
    "crowdsorsing_data.to_csv('final_data/crowdsorsing_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crowdsoursing Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crowdsoursing result\n",
    "cs_result = pd.read_csv('crowdsoursing result aggregated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>which_category_best_fits_this_text_</th>\n",
       "      <th>which_category_best_fits_this_text_:confidence</th>\n",
       "      <th>label</th>\n",
       "      <th>tweets</th>\n",
       "      <th>which_category_best_fits_this_text__gold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2684818958</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>3/13/2020 16:59:27</td>\n",
       "      <td>happy_love_joy</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>excitement</td>\n",
       "      <td>RT @zwkishi: Haven't draw yuzu that much but I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2684818959</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>3/13/2020 16:56:18</td>\n",
       "      <td>excitement</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>excitement</td>\n",
       "      <td>RT @epollsurveys: Happy Friday everyone! ðŸ˜€\\n#M...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2684818960</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>5</td>\n",
       "      <td>3/13/2020 16:33:52</td>\n",
       "      <td>happy_love_joy</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>excitement</td>\n",
       "      <td>@FedExHelp Hi Lauryn! Luckily with some flatte...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2684818961</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>4</td>\n",
       "      <td>3/13/2020 16:28:23</td>\n",
       "      <td>surprise_sad_frustration</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>excitement</td>\n",
       "      <td>@meakoopa My brother made me watch the entire ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2684818962</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>3/13/2020 16:28:39</td>\n",
       "      <td>surprise_sad_frustration</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>excitement</td>\n",
       "      <td>@BeatsNdBang Undefeated! Iâ€™m tryna work with m...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>2686752167</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>angry</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>angry</td>\n",
       "      <td>@tsukasaslilfang only maane will get mad ðŸ˜ </td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>2686758825</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>surprise_sad_frustration</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>surprise</td>\n",
       "      <td>@DannyPaps94 @MeechIsDEAD No I canâ€™t take the ...</td>\n",
       "      <td>surprise_sad_frustration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>2686760556</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happy_love_joy</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>happy</td>\n",
       "      <td>Animals #m#: #Animals#: #Penguins #in #Love #|...</td>\n",
       "      <td>happy_love_joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>2686761386</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>surprise_sad_frustration</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>surprise</td>\n",
       "      <td>why it always da single moms getting spicy ðŸ˜¢</td>\n",
       "      <td>fear_disgust_depression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>2686763230</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>surprise_sad_frustration</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>fear</td>\n",
       "      <td>RT @_simpsss: Iâ€™ve watched almost all categori...</td>\n",
       "      <td>fear_disgust_depression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       _unit_id  _golden _unit_state  _trusted_judgments   _last_judgment_at  \\\n",
       "0    2684818958    False   finalized                   3  3/13/2020 16:59:27   \n",
       "1    2684818959    False   finalized                   3  3/13/2020 16:56:18   \n",
       "2    2684818960    False   finalized                   5  3/13/2020 16:33:52   \n",
       "3    2684818961    False   finalized                   4  3/13/2020 16:28:23   \n",
       "4    2684818962    False   finalized                   3  3/13/2020 16:28:39   \n",
       "..          ...      ...         ...                 ...                 ...   \n",
       "214  2686752167     True      golden                   1                 NaN   \n",
       "215  2686758825     True      golden                   1                 NaN   \n",
       "216  2686760556     True      golden                   1                 NaN   \n",
       "217  2686761386     True      golden                   1                 NaN   \n",
       "218  2686763230     True      golden                   1                 NaN   \n",
       "\n",
       "    which_category_best_fits_this_text_  \\\n",
       "0                        happy_love_joy   \n",
       "1                            excitement   \n",
       "2                        happy_love_joy   \n",
       "3              surprise_sad_frustration   \n",
       "4              surprise_sad_frustration   \n",
       "..                                  ...   \n",
       "214                               angry   \n",
       "215            surprise_sad_frustration   \n",
       "216                      happy_love_joy   \n",
       "217            surprise_sad_frustration   \n",
       "218            surprise_sad_frustration   \n",
       "\n",
       "     which_category_best_fits_this_text_:confidence       label  \\\n",
       "0                                            0.6667  excitement   \n",
       "1                                            0.6667  excitement   \n",
       "2                                            0.6000  excitement   \n",
       "3                                            0.2500  excitement   \n",
       "4                                            0.3333  excitement   \n",
       "..                                              ...         ...   \n",
       "214                                          1.0000       angry   \n",
       "215                                          1.0000    surprise   \n",
       "216                                          1.0000       happy   \n",
       "217                                          1.0000    surprise   \n",
       "218                                          1.0000        fear   \n",
       "\n",
       "                                                tweets  \\\n",
       "0    RT @zwkishi: Haven't draw yuzu that much but I...   \n",
       "1    RT @epollsurveys: Happy Friday everyone! ðŸ˜€\\n#M...   \n",
       "2    @FedExHelp Hi Lauryn! Luckily with some flatte...   \n",
       "3    @meakoopa My brother made me watch the entire ...   \n",
       "4    @BeatsNdBang Undefeated! Iâ€™m tryna work with m...   \n",
       "..                                                 ...   \n",
       "214         @tsukasaslilfang only maane will get mad ðŸ˜    \n",
       "215  @DannyPaps94 @MeechIsDEAD No I canâ€™t take the ...   \n",
       "216  Animals #m#: #Animals#: #Penguins #in #Love #|...   \n",
       "217       why it always da single moms getting spicy ðŸ˜¢   \n",
       "218  RT @_simpsss: Iâ€™ve watched almost all categori...   \n",
       "\n",
       "    which_category_best_fits_this_text__gold  \n",
       "0                                        NaN  \n",
       "1                                        NaN  \n",
       "2                                        NaN  \n",
       "3                                        NaN  \n",
       "4                                        NaN  \n",
       "..                                       ...  \n",
       "214                                    angry  \n",
       "215                 surprise_sad_frustration  \n",
       "216                           happy_love_joy  \n",
       "217                  fear_disgust_depression  \n",
       "218                  fear_disgust_depression  \n",
       "\n",
       "[219 rows x 10 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_sorted = cs_result[['tweets', 'which_category_best_fits_this_text_', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesyang/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "cs_sorted[cs_sorted.which_category_best_fits_this_text_ == 'happy_love_joy'].which_category_best_fits_this_text_ = 'happy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_label = []\n",
    "for label in cs_sorted.which_category_best_fits_this_text_.values:\n",
    "    if label == 'happy_love_joy':\n",
    "        cs_label.append('happy') \n",
    "    elif label == 'surprise_sad_frustration':\n",
    "        cs_label.append('surprise')\n",
    "    elif label == 'fear_disgust_depression':\n",
    "        cs_label.append('fear')\n",
    "    else:\n",
    "        cs_label.append(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesyang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "cs_sorted['cs_label'] = np.array(cs_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  0.4931506849315068\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy is: ', sum(cs_sorted.label == cs_sorted.cs_label)/len(cs_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "#Code from Lab4 Text as Data\n",
    "#Train and Evaluate Classifiers\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def evaluation_summary(description, predictions, true_labels):\n",
    "    print(\"Evaluation for: \" + description)\n",
    "    precision = precision_score(true_labels, predictions, average='macro')\n",
    "    recall = recall_score(true_labels, predictions, average='macro')\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    f1 = fbeta_score(true_labels, predictions, 1, average='macro') #1 means f_1 measure\n",
    "    print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f\" % (description,accuracy,precision,recall,f1))\n",
    "    # Specify three digits instead of the default two.\n",
    "    print(classification_report(true_labels, predictions, digits=3))\n",
    "    print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions)) # Note the order here is true, predicted, odd.\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for: Crowdsoursing Result\n",
      "Classifier 'Crowdsoursing Result' has Acc=0.493 P=0.487 R=0.523 F1=0.452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry      0.833     0.909     0.870        33\n",
      "  excitement      0.205     0.500     0.291        16\n",
      "        fear      0.242     0.667     0.356        12\n",
      "       happy      0.775     0.383     0.512        81\n",
      "    pleasant      0.086     0.250     0.128        12\n",
      "    surprise      0.778     0.431     0.554        65\n",
      "\n",
      "    accuracy                          0.493       219\n",
      "   macro avg      0.487     0.523     0.452       219\n",
      "weighted avg      0.676     0.493     0.533       219\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[30  2  1  0  0  0]\n",
      " [ 0  8  2  2  4  0]\n",
      " [ 0  0  8  0  0  4]\n",
      " [ 1 18  2 31 26  3]\n",
      " [ 0  2  2  4  3  1]\n",
      " [ 5  9 18  3  2 28]]\n"
     ]
    }
   ],
   "source": [
    "(accuracy, \n",
    " precision, \n",
    " recall, f1) = evaluation_summary(\"Crowdsoursing Result\", \n",
    "                                  cs_sorted.label, \n",
    "                                  cs_sorted.cs_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90909091, 0.5       , 0.66666667, 0.38271605, 0.25      ,\n",
       "       0.43076923])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#category accuracy\n",
    "#Accuracy of each class\n",
    "#Get the confusion matrix\n",
    "cm = confusion_matrix(cs_sorted.cs_label, cs_sorted.label)\n",
    "\n",
    "#Now the normalize the diagonal entries\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "#The diagonal entries are the accuracies of each class\n",
    "cm.diagonal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform into TFIDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text used here is clean tweets after lemmed\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(final_tweets_all.clean_tweets_lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44132, 31671)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9580)\t0.3110453537225645\n",
      "  (0, 29144)\t0.17430504145521622\n",
      "  (0, 12253)\t0.1885678357004289\n",
      "  (0, 1628)\t0.292145807122746\n",
      "  (0, 25204)\t0.3578637878051896\n",
      "  (0, 23821)\t0.3578637878051896\n",
      "  (0, 27407)\t0.07527768987593568\n",
      "  (0, 2034)\t0.12857150606289064\n",
      "  (0, 11895)\t0.18978773758013692\n",
      "  (0, 11642)\t0.1782031791803431\n",
      "  (0, 27588)\t0.18442379861656227\n",
      "  (0, 10287)\t0.146983526989967\n",
      "  (0, 31140)\t0.22529821072501083\n",
      "  (0, 7598)\t0.29552586184894863\n",
      "  (0, 21300)\t0.34715327469268836\n",
      "  (0, 25078)\t0.22712418845821622\n",
      "  (0, 20189)\t0.1794375224634039\n",
      "  (0, 23520)\t0.08857598825641594\n",
      "  (1, 19492)\t0.21480289229445185\n",
      "  (1, 23205)\t0.25619013103408045\n",
      "  (1, 2037)\t0.41082751466390827\n",
      "  (1, 8903)\t0.39570669800239056\n",
      "  (1, 19883)\t0.1701498861248553\n",
      "  (1, 21263)\t0.2667202338450074\n",
      "  (1, 21771)\t0.4083759478931913\n",
      "  :\t:\n",
      "  (44128, 17412)\t0.1334005041656718\n",
      "  (44128, 27853)\t0.1017247432955413\n",
      "  (44128, 4309)\t0.14846495698106463\n",
      "  (44128, 27407)\t0.0989728339899038\n",
      "  (44128, 27588)\t0.12123742130202816\n",
      "  (44129, 16462)\t0.6003183337124426\n",
      "  (44129, 25666)\t0.427026383433459\n",
      "  (44129, 31132)\t0.3257972375707401\n",
      "  (44129, 29412)\t0.45883649102798624\n",
      "  (44129, 10018)\t0.24184364386022314\n",
      "  (44129, 25458)\t0.24503200169985934\n",
      "  (44129, 2783)\t0.14853474549823656\n",
      "  (44130, 6708)\t0.6956262232098672\n",
      "  (44130, 8964)\t0.3168138382146505\n",
      "  (44130, 30213)\t0.44400907941553747\n",
      "  (44130, 5033)\t0.4284477448048727\n",
      "  (44130, 23520)\t0.18714063392716912\n",
      "  (44131, 10224)\t0.659087072435572\n",
      "  (44131, 15634)\t0.3818082302256703\n",
      "  (44131, 28530)\t0.3967078937819176\n",
      "  (44131, 23954)\t0.29673808398380724\n",
      "  (44131, 13312)\t0.21820249013456358\n",
      "  (44131, 14661)\t0.24693790186430298\n",
      "  (44131, 27407)\t0.16222424257001591\n",
      "  (44131, 27588)\t0.1987176485607124\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
